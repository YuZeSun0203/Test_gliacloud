{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "# step1:建立一個隱藏層的神經網路\n",
    "def build_model(): \n",
    "    # 輸入層\n",
    "    input_img = Input(shape=(224,224,3))\n",
    "    # 卷積層(一層隱藏層)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(input_img)\n",
    "    # 輸出層\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "    return model\n",
    "\n",
    "model =  build_model()\n",
    "model.summary()\n",
    "\n",
    "# step2:compile設定，包含loss function\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.當資料量不平均時，我們會使用recall及precision所計算之f1來取代accuracy,以判斷模型優劣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.因為在神經網路中，為解決複雜問題，我們會引入非線性激活函數，如:早期的sigmoid、tanh;最近的relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.bias:訓練資料中與真值的差\n",
    "  variance:與未知資料的差異\n",
    "  例如:overfitting時，會產生低偏差、高變異的狀況"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.從概念上來理解，是將類別/數值轉向量(由0跟1組成)\n",
    "  從功能上來理解，可提高計算機運算效率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.可透過下列方式避免overfitting\n",
    "1.提高數據量\n",
    "2.選擇合適的模型(層數↓、訓練時間↓、噪音↑、資料正規化L1、L2)\n",
    "3.選擇集成模型(bagging、boosting、dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2-1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自然語言處理套件\n",
    "from keras.preprocessing import sequence,text #數字套件、文字套件\n",
    "from keras.preprocessing.text import Tokenizer # 建立token字典\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize # 將句子轉換成單詞\n",
    "from nltk import FreqDist #把不重複單詞變成字典\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer # #詞性還原\n",
    "stemmer=SnowballStemmer('english') #\n",
    "lemma=WordNetLemmatizer() \n",
    "from string import punctuation #\n",
    "import re#正規表達式\n",
    "\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('wordnet') #\n",
    "nltk.download('punkt') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw = pd.read_csv('raw_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No ,  he says now .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And what did he do ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The money 's there .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That was less than a year ago .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But he made only the first .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Phrase\n",
       "0              No ,  he says now .\n",
       "1             And what did he do ?\n",
       "2             The money 's there .\n",
       "3  That was less than a year ago .\n",
       "4     But he made only the first ."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' No ,  he says now .', 'And what did he do ?',\n",
       "       \" The money 's there .\", ...,\n",
       "       ' Where people get their money and how they get it .',\n",
       "       'It may also be the last .', 'Only one in five said no .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取值raw['Phrase']成為series 以利nltk處理\n",
    "phrase_series = raw.Phrase.values\n",
    "# 觀察結果\n",
    "phrase_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_probs(phrase):\n",
    "    new_phrase_list=[]\n",
    "    for i in range(0,len(phrase)):\n",
    "      # 調整1:轉型字串\n",
    "        new_phrase=str(phrase[i]) \n",
    "        \n",
    "      # 調整2:將new_phrase轉型字串後改為小寫，並用word_tokenize切分單詞後一個一個取出，最後以lemma.lemmatize()將詞性還原  \n",
    "        new_phrase=[lemma.lemmatize(w) for w in word_tokenize(str(new_phrase).lower())]\n",
    "        \n",
    "      # 調整3:append到new_phrase_list，並回傳\n",
    "        new_phrase_list.append(new_phrase)\n",
    "        \n",
    "      # 調整4:計算bigram_probs、trigram_probs\n",
    "        bigram_probs=ngrams(New_Phrase,2)\n",
    "        trigram_probs=ngrams(token,3)\n",
    "    return bigram_probs, trigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt2, cnt3  = ngram_probs(phrase_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_dic = {}\n",
    "trigrams_dic = {}\n",
    "\n",
    "filename = 'raw_sentences.txt'    \n",
    "with open(filename, \"r\") as fp:\n",
    "    for line in fp:\n",
    "        \n",
    "        words = line.split()\n",
    "#         print(words)\n",
    "\n",
    "        bigrams = ngrams(words,2)\n",
    "#         print(list(bigrams))   \n",
    "\n",
    "        trigrams = ngrams(words,3)\n",
    "#         print(list(trigrams))\n",
    "        \n",
    "        \n",
    "        for bigram in bigrams:\n",
    "            bigram = word.lower()\n",
    "            \n",
    "            if bigram not in bigrams_dic:\n",
    "                bigrams_dic[bigram] = 1\n",
    "            else:\n",
    "                bigrams_dic[bigram] += 1\n",
    "                \n",
    "        for trigram in trigrams:\n",
    "            trigram = word.lower()\n",
    "            \n",
    "            if trigram not in trigrams_dic:\n",
    "                trigrams_dic[trigram] = 1\n",
    "            else:\n",
    "                trigrams_dic[trigram] += 1\n",
    "\n",
    "\n",
    "total_bigrams = sum(bigrams_dic.values())\n",
    "total_trigrams = sum(trigrams_dic.values())\n",
    "\n",
    "\n",
    "output_file = 'bigrams.txt'\n",
    "with open(output_file, \"w\") as fs:\n",
    "    for key, value in sorted(bigrams_dic.items()):\n",
    "        probability = value / total_words\n",
    "        fs.write(key + \": \" + str(probability) + \"\\n\")\n",
    "        \n",
    "output_file = 'trigrams.txt'\n",
    "with open(output_file, \"w\") as fs:\n",
    "    for key, value in sorted(trigrams_dic.items()):\n",
    "        probability = value / total_words\n",
    "        fs.write(key + \": \" + str(probability) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2-3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
